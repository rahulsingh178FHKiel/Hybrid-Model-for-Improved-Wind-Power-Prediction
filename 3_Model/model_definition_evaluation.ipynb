{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "# Import models you're considering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "For improved wind power prediction, we consider the following models:\n",
    "- **Random Forest Regressor:** Handles non-linear relationships and is robust to outliers and feature interactions.\n",
    "- **Gradient Boosting Regressor (e.g., XGBoost):** Known for high predictive performance and ability to handle complex patterns.\n",
    "- **LSTM Neural Network:** Suitable for time series data, capturing temporal dependencies.\n",
    "\n",
    "We select Random Forest and Gradient Boosting for their interpretability and strong performance on tabular data, and LSTM for its ability to model sequential dependencies. This allows us to compare classical ensemble methods with deep learning approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We expand feature engineering beyond the baseline by including:\n",
    "- **Wind forecasts:** (`u`, `v`, `ws`, `wd`) from all wind farms (wf1-wf7).\n",
    "- **Lag features:** Previous values of power output (e.g., `wp1` lagged by 1, 2, 3 hours).\n",
    "- **Rolling statistics:** Rolling mean and std of `wp1` over past 3/6/12 hours.\n",
    "- **Time features:** Hour of day, day of week, and month to capture temporal patterns.\n",
    "\n",
    "These features are expected to improve model performance by providing more context and capturing temporal and spatial dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "# Load train.csv\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Load and merge all windforecasts files from wf/\n",
    "wf_folder = 'wf'\n",
    "wf_files = sorted(glob.glob(os.path.join(wf_folder, 'windforecasts_wf*.csv')))\n",
    "\n",
    "wf_dfs = []\n",
    "for i, f in enumerate(wf_files, 1):\n",
    "    df = pd.read_csv(f)\n",
    "    df = df.rename(columns={col: f\"{col}_wf{i}\" for col in df.columns if col not in ['date', 'hors']})\n",
    "    wf_dfs.append(df)\n",
    "\n",
    "if wf_dfs:\n",
    "    wf_merged = reduce(lambda left, right: pd.merge(left, right, on=['date', 'hors'], how='outer'), wf_dfs)\n",
    "else:\n",
    "    wf_merged = pd.DataFrame()\n",
    "\n",
    "# Merge train and wind forecasts on 'date' and 'hors' if both exist, else just on 'date'\n",
    "merge_cols = ['date']\n",
    "if 'hors' in train.columns and 'hors' in wf_merged.columns:\n",
    "    merge_cols.append('hors')\n",
    "df = pd.merge(train, wf_merged, on=merge_cols, how='left')\n",
    "\n",
    "# Feature engineering\n",
    "# Lag features for wp1\n",
    "for lag in [1, 2, 3]:\n",
    "    df[f'wp1_lag{lag}'] = df['wp1'].shift(lag)\n",
    "# Rolling mean and std for wp1\n",
    "for window in [3, 6, 12]:\n",
    "    df[f'wp1_rollmean{window}'] = df['wp1'].rolling(window).mean()\n",
    "    df[f'wp1_rollstd{window}'] = df['wp1'].rolling(window).std()\n",
    "# Time features\n",
    "if 'date' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['date'].astype(str), format='%Y%m%d%H')\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Drop rows with NaN from feature engineering\n",
    "feature_cols = [col for col in df.columns if col not in ['target_variable', 'datetime']]\n",
    "df = df.dropna(subset=feature_cols)\n",
    "\n",
    "# Feature and target variable selection\n",
    "X = df[[col for col in df.columns if col not in ['wp1', 'target_variable', 'datetime']]]\n",
    "y = df['wp1'].shift(-1).dropna()  # Next-hour prediction\n",
    "X = X.iloc[:-1, :]  # Align X and y\n",
    "\n",
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We apply hyperparameter tuning to optimize model performance. For Random Forest and Gradient Boosting, we use GridSearchCV to search over key parameters (e.g., number of estimators, max depth, learning rate). For LSTM, we tune the number of layers, units, and dropout rate using KerasTuner or manual search.\n",
    "\n",
    "Grid search is chosen for its thoroughness, while random search or Bayesian optimization can be considered for larger parameter spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF params: {'max_depth': 10, 'n_estimators': 200}\n",
      "Best GB params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best GB params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Random Forest and Gradient Boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example: Random Forest\n",
    "rf_params = {'n_estimators': [100, 200], 'max_depth': [5, 10, None]}\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "gs_rf = GridSearchCV(rf, rf_params, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "gs_rf.fit(X_train, y_train)\n",
    "print(f\"Best RF params: {gs_rf.best_params_}\")\n",
    "\n",
    "# Example: Gradient Boosting\n",
    "gb_params = {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]}\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "gs_gb = GridSearchCV(gb, gb_params, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "gs_gb.fit(X_train, y_train)\n",
    "print(f\"Best GB params: {gs_gb.best_params_}\")\n",
    "\n",
    "# For LSTM, tuning is typically done separately due to Keras API differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Hyperparameter Tuning (Manual Example)\n",
    "\n",
    "LSTM hyperparameter tuning is typically done manually or with specialized tools like KerasTuner. Below is an example of how you might manually tune key parameters such as the number of LSTM units, dropout rate, and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step\n",
      "units=32, dropout=0.1, epochs=10 => MAE: 0.0441\n",
      "units=32, dropout=0.1, epochs=10 => MAE: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step\n",
      "units=32, dropout=0.1, epochs=20 => MAE: 0.0278\n",
      "units=32, dropout=0.1, epochs=20 => MAE: 0.0278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step\n",
      "units=32, dropout=0.2, epochs=10 => MAE: 0.0523\n",
      "units=32, dropout=0.2, epochs=10 => MAE: 0.0523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step\n",
      "units=32, dropout=0.2, epochs=20 => MAE: 0.0591\n",
      "units=32, dropout=0.2, epochs=20 => MAE: 0.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step\n",
      "units=64, dropout=0.1, epochs=10 => MAE: 0.0271\n",
      "units=64, dropout=0.1, epochs=10 => MAE: 0.0271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step\n",
      "units=64, dropout=0.1, epochs=20 => MAE: 0.0224\n",
      "units=64, dropout=0.1, epochs=20 => MAE: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step\n",
      "units=64, dropout=0.2, epochs=10 => MAE: 0.0404\n",
      "units=64, dropout=0.2, epochs=10 => MAE: 0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step\n",
      "units=64, dropout=0.2, epochs=20 => MAE: 0.0365\n",
      "Best LSTM params: units=64, dropout=0.1, epochs=20, MAE=0.0224\n",
      "units=64, dropout=0.2, epochs=20 => MAE: 0.0365\n",
      "Best LSTM params: units=64, dropout=0.1, epochs=20, MAE=0.0224\n"
     ]
    }
   ],
   "source": [
    "# Example: Manual LSTM hyperparameter tuning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale features for LSTM\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "timesteps = 3\n",
    "X_lstm, y_lstm = [], []\n",
    "for i in range(timesteps, len(X_scaled)):\n",
    "    X_lstm.append(X_scaled[i-timesteps:i, :])\n",
    "    y_lstm.append(y_scaled[i])\n",
    "X_lstm, y_lstm = np.array(X_lstm), np.array(y_lstm)\n",
    "\n",
    "split_idx = int(0.8 * len(X_lstm))\n",
    "X_train_lstm, X_test_lstm = X_lstm[:split_idx], X_lstm[split_idx:]\n",
    "y_train_lstm, y_test_lstm = y_lstm[:split_idx], y_lstm[split_idx:]\n",
    "\n",
    "# Try different hyperparameters\n",
    "best_mae = float('inf')\n",
    "best_params = None\n",
    "for units in [32, 64]:\n",
    "    for dropout in [0.1, 0.2]:\n",
    "        for epochs in [10, 20]:\n",
    "            model = Sequential([\n",
    "                LSTM(units, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=False),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu'),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "            model.fit(X_train_lstm, y_train_lstm, epochs=epochs, batch_size=32, validation_split=0.2, verbose=0)\n",
    "            y_pred = model.predict(X_test_lstm)\n",
    "            y_pred_inv = scaler_y.inverse_transform(y_pred)\n",
    "            y_test_inv = scaler_y.inverse_transform(y_test_lstm)\n",
    "            mae = np.mean(np.abs(y_test_inv - y_pred_inv))\n",
    "            print(f\"units={units}, dropout={dropout}, epochs={epochs} => MAE: {mae:.4f}\")\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_params = (units, dropout, epochs)\n",
    "print(f\"Best LSTM params: units={best_params[0]}, dropout={best_params[1]}, epochs={best_params[2]}, MAE={best_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We implement the final models using the best hyperparameters found. Both Random Forest and Gradient Boosting are trained and evaluated. (LSTM implementation can be added in a separate section if desired.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MAE: 0.0071, RMSE: 0.0351\n",
      "Gradient Boosting MAE: 0.0063, RMSE: 0.0351\n",
      "Gradient Boosting MAE: 0.0063, RMSE: 0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m28,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,041</span> (121.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,041\u001b[0m (121.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,041</span> (121.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,041\u001b[0m (121.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 812us/step - loss: 0.0060 - mae: 0.0442 - val_loss: 0.0043 - val_mae: 0.0233\n",
      "Epoch 2/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 812us/step - loss: 0.0060 - mae: 0.0442 - val_loss: 0.0043 - val_mae: 0.0233\n",
      "Epoch 2/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768us/step - loss: 0.0028 - mae: 0.0259 - val_loss: 0.0041 - val_mae: 0.0230\n",
      "Epoch 3/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768us/step - loss: 0.0028 - mae: 0.0259 - val_loss: 0.0041 - val_mae: 0.0230\n",
      "Epoch 3/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 764us/step - loss: 0.0025 - mae: 0.0230 - val_loss: 0.0040 - val_mae: 0.0215\n",
      "Epoch 4/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 764us/step - loss: 0.0025 - mae: 0.0230 - val_loss: 0.0040 - val_mae: 0.0215\n",
      "Epoch 4/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 764us/step - loss: 0.0027 - mae: 0.0234 - val_loss: 0.0046 - val_mae: 0.0332\n",
      "Epoch 5/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 764us/step - loss: 0.0027 - mae: 0.0234 - val_loss: 0.0046 - val_mae: 0.0332\n",
      "Epoch 5/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792us/step - loss: 0.0026 - mae: 0.0224 - val_loss: 0.0038 - val_mae: 0.0197\n",
      "Epoch 6/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792us/step - loss: 0.0026 - mae: 0.0224 - val_loss: 0.0038 - val_mae: 0.0197\n",
      "Epoch 6/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 760us/step - loss: 0.0025 - mae: 0.0216 - val_loss: 0.0046 - val_mae: 0.0337\n",
      "Epoch 7/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 760us/step - loss: 0.0025 - mae: 0.0216 - val_loss: 0.0046 - val_mae: 0.0337\n",
      "Epoch 7/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 766us/step - loss: 0.0025 - mae: 0.0218 - val_loss: 0.0039 - val_mae: 0.0216\n",
      "Epoch 8/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 766us/step - loss: 0.0025 - mae: 0.0218 - val_loss: 0.0039 - val_mae: 0.0216\n",
      "Epoch 8/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 767us/step - loss: 0.0023 - mae: 0.0205 - val_loss: 0.0038 - val_mae: 0.0194\n",
      "Epoch 9/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 767us/step - loss: 0.0023 - mae: 0.0205 - val_loss: 0.0038 - val_mae: 0.0194\n",
      "Epoch 9/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 759us/step - loss: 0.0023 - mae: 0.0205 - val_loss: 0.0040 - val_mae: 0.0268\n",
      "Epoch 10/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 759us/step - loss: 0.0023 - mae: 0.0205 - val_loss: 0.0040 - val_mae: 0.0268\n",
      "Epoch 10/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786us/step - loss: 0.0022 - mae: 0.0198 - val_loss: 0.0039 - val_mae: 0.0263\n",
      "Epoch 11/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786us/step - loss: 0.0022 - mae: 0.0198 - val_loss: 0.0039 - val_mae: 0.0263\n",
      "Epoch 11/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 760us/step - loss: 0.0023 - mae: 0.0203 - val_loss: 0.0037 - val_mae: 0.0199\n",
      "Epoch 12/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 760us/step - loss: 0.0023 - mae: 0.0203 - val_loss: 0.0037 - val_mae: 0.0199\n",
      "Epoch 12/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765us/step - loss: 0.0022 - mae: 0.0189 - val_loss: 0.0039 - val_mae: 0.0257\n",
      "Epoch 13/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765us/step - loss: 0.0022 - mae: 0.0189 - val_loss: 0.0039 - val_mae: 0.0257\n",
      "Epoch 13/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 758us/step - loss: 0.0022 - mae: 0.0186 - val_loss: 0.0037 - val_mae: 0.0200\n",
      "Epoch 14/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 758us/step - loss: 0.0022 - mae: 0.0186 - val_loss: 0.0037 - val_mae: 0.0200\n",
      "Epoch 14/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step - loss: 0.0022 - mae: 0.0187 - val_loss: 0.0037 - val_mae: 0.0159\n",
      "Epoch 15/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step - loss: 0.0022 - mae: 0.0187 - val_loss: 0.0037 - val_mae: 0.0159\n",
      "Epoch 15/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 758us/step - loss: 0.0021 - mae: 0.0186 - val_loss: 0.0039 - val_mae: 0.0226\n",
      "Epoch 16/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 758us/step - loss: 0.0021 - mae: 0.0186 - val_loss: 0.0039 - val_mae: 0.0226\n",
      "Epoch 16/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 766us/step - loss: 0.0021 - mae: 0.0177 - val_loss: 0.0037 - val_mae: 0.0210\n",
      "Epoch 17/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 766us/step - loss: 0.0021 - mae: 0.0177 - val_loss: 0.0037 - val_mae: 0.0210\n",
      "Epoch 17/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765us/step - loss: 0.0021 - mae: 0.0183 - val_loss: 0.0039 - val_mae: 0.0238\n",
      "Epoch 18/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765us/step - loss: 0.0021 - mae: 0.0183 - val_loss: 0.0039 - val_mae: 0.0238\n",
      "Epoch 18/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794us/step - loss: 0.0020 - mae: 0.0178 - val_loss: 0.0038 - val_mae: 0.0239\n",
      "Epoch 19/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794us/step - loss: 0.0020 - mae: 0.0178 - val_loss: 0.0038 - val_mae: 0.0239\n",
      "Epoch 19/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800us/step - loss: 0.0020 - mae: 0.0178 - val_loss: 0.0038 - val_mae: 0.0222\n",
      "Epoch 20/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800us/step - loss: 0.0020 - mae: 0.0178 - val_loss: 0.0038 - val_mae: 0.0222\n",
      "Epoch 20/20\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814us/step - loss: 0.0020 - mae: 0.0174 - val_loss: 0.0038 - val_mae: 0.0235\n",
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814us/step - loss: 0.0020 - mae: 0.0174 - val_loss: 0.0038 - val_mae: 0.0235\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396us/step\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396us/step\n",
      "LSTM MAE: 0.0215, RMSE: 0.0583\n",
      "               Model       MAE      RMSE\n",
      "0      Random Forest  0.007088  0.035106\n",
      "1  Gradient Boosting  0.006257  0.035123\n",
      "2               LSTM  0.021484  0.058280\n",
      "LSTM MAE: 0.0215, RMSE: 0.0583\n",
      "               Model       MAE      RMSE\n",
      "0      Random Forest  0.007088  0.035106\n",
      "1  Gradient Boosting  0.006257  0.035123\n",
      "2               LSTM  0.021484  0.058280\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the best Random Forest and Gradient Boosting models\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Random Forest\n",
    "rf_best = gs_rf.best_estimator_\n",
    "rf_best.fit(X_train, y_train)\n",
    "y_pred_rf = rf_best.predict(X_test)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "print(f\"Random Forest MAE: {mae_rf:.4f}, RMSE: {rmse_rf:.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_best = gs_gb.best_estimator_\n",
    "gb_best.fit(X_train, y_train)\n",
    "y_pred_gb = gb_best.predict(X_test)\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "print(f\"Gradient Boosting MAE: {mae_gb:.4f}, RMSE: {rmse_gb:.4f}\")\n",
    "\n",
    "# (Optional) LSTM implementation for time series modeling\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale features for LSTM\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Reshape for LSTM: (samples, timesteps, features)\n",
    "timesteps = 3  # Use last 3 hours for prediction\n",
    "X_lstm = []\n",
    "y_lstm = []\n",
    "for i in range(timesteps, len(X_scaled)):\n",
    "    X_lstm.append(X_scaled[i-timesteps:i, :])\n",
    "    y_lstm.append(y_scaled[i])\n",
    "X_lstm, y_lstm = np.array(X_lstm), np.array(y_lstm)\n",
    "\n",
    "# Train/test split for LSTM (keep same ratio as before)\n",
    "split_idx = int(0.8 * len(X_lstm))\n",
    "X_train_lstm, X_test_lstm = X_lstm[:split_idx], X_lstm[split_idx:]\n",
    "y_train_lstm, y_test_lstm = y_lstm[:split_idx], y_lstm[split_idx:]\n",
    "\n",
    "# Build LSTM model\n",
    "try:\n",
    "    units, dropout, epochs = best_params\n",
    "except NameError:\n",
    "    units, dropout, epochs = 64, 0.2, 20  # fallback to defaults if not tuned\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(units, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=False),\n",
    "    Dropout(dropout),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train LSTM\n",
    "history_lstm = lstm_model.fit(X_train_lstm, y_train_lstm, epochs=epochs, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate LSTM\n",
    "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "y_pred_lstm_inv = scaler_y.inverse_transform(y_pred_lstm)\n",
    "y_test_lstm_inv = scaler_y.inverse_transform(y_test_lstm)\n",
    "\n",
    "mae_lstm = np.mean(np.abs(y_test_lstm_inv - y_pred_lstm_inv))\n",
    "rmse_lstm = np.sqrt(np.mean((y_test_lstm_inv - y_pred_lstm_inv) ** 2))\n",
    "print(f\"LSTM MAE: {mae_lstm:.4f}, RMSE: {rmse_lstm:.4f}\")\n",
    "\n",
    "# Add LSTM results to the comparison table\n",
    "results.loc[len(results)] = ['LSTM', mae_lstm, rmse_lstm]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We use the following metrics to evaluate model performance:\n",
    "- **Mean Absolute Error (MAE):** Measures average magnitude of errors, easy to interpret.\n",
    "- **Root Mean Squared Error (RMSE):** Penalizes larger errors more, useful for highlighting significant prediction mistakes.\n",
    "\n",
    "These metrics are standard for regression tasks in wind power forecasting and allow direct comparison with the baseline model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model       MAE      RMSE\n",
      "0      Random Forest  0.007088  0.035106\n",
      "1  Gradient Boosting  0.006257  0.035123\n",
      "2               LSTM  0.021484  0.058280\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all models and compare\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting', 'LSTM'],\n",
    "    'MAE': [mae_rf, mae_gb, mae_lstm],\n",
    "    'RMSE': [rmse_rf, rmse_gb, rmse_lstm]\n",
    "})\n",
    "print(results)\n",
    "\n",
    "# (Optional) Add baseline results for comparison if available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "The improved models (Random Forest and Gradient Boosting) are compared against the baseline (MLP and Linear Regression). Both ensemble models are expected to outperform the baseline due to their ability to capture complex relationships and utilize more features. If the improvement is marginal or negative, possible reasons include overfitting, insufficient feature engineering, or the need for more advanced time series models (e.g., LSTM).\n",
    "\n",
    "Summarize the results in a table and discuss any observed improvements or setbacks, referencing the evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model       MAE      RMSE\n",
      "0      Random Forest  0.007088  0.035106\n",
      "1  Gradient Boosting  0.006257  0.035123\n",
      "2               LSTM  0.021484  0.058280\n"
     ]
    }
   ],
   "source": [
    "# Comparative Analysis code\n",
    "# Example: comparing all models (add baseline results if available)\n",
    "# baseline_mae, baseline_rmse = ... (from previous notebook)\n",
    "# results.loc[len(results)] = ['Baseline MLP', baseline_mae, baseline_rmse]\n",
    "# results.loc[len(results)] = ['Linear Regression', baseline_mae_lr, baseline_rmse_lr]\n",
    "print(results)\n",
    "\n",
    "# Discuss improvements or setbacks in markdown cell above\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
